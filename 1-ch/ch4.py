# -*- coding: utf-8 -*-
"""4장.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18csmBL_vM5TP3cR2ysiDB10DFaO0vjLN

오차제곱합
"""

import numpy as np

#오차제곱합 함수 정의
def sum_squares_error(y,t):
    return 0.5*np.sum((y-t)**2)

#정답 : 2
t = [0,0,1,0,0,0,0,0,0,0]  #정답 레이블 (원-핫 인코딩 표기됨)

#예1
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
print(sum_squares_error(np.array(y), np.array(t)))

#예2
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
print(sum_squares_error(np.array(y), np.array(t)))

"""교차 엔트로피 오차"""

def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t*np.log(y+delta))

#위와 똑같은 예
t = [0,0,1,0,0,0,0,0,0,0]

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
print(cross_entropy_error(np.array(y), np.array(t)))

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
print(cross_entropy_error(np.array(y), np.array(t)))

"""미니배치 학습"""

from google.colab import drive
drive.mount('/content/drive')

import sys, os
sys.path.append("/content/drive/MyDrive/Colab Notebooks/밑바닥딥러닝/dataset")

from mnist import load_mnist

(X_train, t_train), (X_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

print(X_train.shape)
print(t_train.shape)

train_size = X_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)  #60000미만의 수에서 무작위로 10개 추출
X_batch = X_train[batch_mask]  #batch_mask인덱스의 X_train값
t_batch = t_train[batch_mask]

"""배치용 교차 엔트로피 오차 구현"""

def cross_entropy_error(y,t):
    if y.ndim ==1:  #y가 1차원인 경우, 데이터 하나씩 오차 구하는 경우
      t = t.reshape(1, t.size)  #2차원으로 바꿔줌
      y = y.reshape(1, y.size)

    batch_size = y.shape[0]  #y가 1차원이 아닌 경우, y의 행 수가 배치 사이즈
    return -np.sum(t*np.log(y+1e-7))/batch_size  #배치 사이즈로 나눔으로써 정규화 -> 데이터 1개의 평균 교차 엔트로피 오차

#정답 레이블이 원-핫 인코딩이 아니라 숫자 레이블로 주어졌을 때
def cross_entropy_error(y, t):
    if y.ndim == 1:
      t = t.reshape(1, t.size)
      y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arrange(batch_size), t]+1e-7))/batch_size

"""수치 미분"""

def numerical_diff(f, x):
    h = 1e-4 #0.0001
    return (f(x+h) - f(x-h)) / (2*h)

#수치미분 예
def funtion_1(x):
    return 0.01*x**2 + 0.1*x

print(numerical_diff(funtion_1, 5))
print(numerical_diff(funtion_1, 10))

#편미분
def funtion_2(x):
    return x[0]**2 + x[1]**2

#문제 1: x0 =3, x1=4일 때 x0에 대한 편미분 구하라
def funtion_tmp1(x0):
    return x0*x0 +4.0**2.0  #변수가 x0하나인 식으로 수정

print(numerical_diff(funtion_tmp1, 3.0))

#문제 2: x0 = 3, x1=4일 때 x1에 대한 편미분 구하라
def funtion_tmp2(x1):
    return 3.0**2.0 + x1*x1

print(numerical_diff(funtion_tmp2, 4.0))

"""기울기"""

def numerical_gradient(f,x):
    h = 1e-4
    grad = np.zeros_like(x) #x와 형상이 같은 배열을 생성

    for idx in range(x.size):
        tmp_val = x[idx]
        #f(x+h)계산
        x[idx] = tmp_val +h
        fxh1 = f(x)

        #f(x-h) 계산
        x[idx] = tmp_val -h
        fxh2 = f(x)

        grad[idx] = (fxh1-fxh2) / (2*h)
        x[idx] = tmp_val #값 복원

    return grad

#예시
numerical_gradient(funtion_2, np.array([3.0, 4.0]))

#경사 하강법
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient(f,x)
        x -= lr*grad
    return x

#예시
def funtion_2(x):
    return x[0]**2 +x[1]**2

init_x = np.array([-3.0, 4.0])
gradient_descent(funtion_2, init_x = init_x, lr = 0.1, step_num=100)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks/밑바닥딥러닝

#신경망에서의 기울기
#간단한 신경망 예시로 기울기 구하는 코드 구현
import sys, os
sys.path.append("/content/drive/MyDrive/Colab Notebooks/밑바닥딥러닝/common")
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) #가중치 매개변수 초기값

    def predict(self, x):  #x :입력 데이터
        return np.dot(x, self.W)

    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)  #t:정답 레이블

        return loss

net = simpleNet()
print(net.W)  #가중치 매개변수

x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
print(np.argmax(p))  #최댓값의 인덱스

t = np.array([0,0,1])  #정답 레이블
net.loss(x, t)  #손실함수 값

def f(W):
    return net.loss(x, t)

dW = numerical_gradient(f, net.W)
print(dW)  #기울기

"""학습 알고리즘 구현하기"""

import sys, os
sys.path.append("/content/drive/MyDrive/Colab Notebooks/밑바닥딥러닝/common")
from common.functions import *
from common.gradient import numerical_gradient

#2층 신경망 클래스 구현하기
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        #가중치 초기화
        self.params = {}  #신경망의 매개변수 보관하는 딕셔너리 변수
        self.params['W1'] = weight_init_std * \
                            np.random.randn(input_size, hidden_size)  #정규분포를 따르는 난수로 초기화
        self.params['b1'] = np.zeros(hidden_size)  #0으로 초기화
        self.params['W2'] =  weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):  #예측 수행
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']

        a1 = np.dot(x,W1) +b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) +b2
        y = softmax(a2)

        return y

    # x:입력 데이터, t: 정답 레이블
    def loss(self, x, t):  #손실 함수 값
        y = self.predict(x)

        return cross_entropy_error(y, t)

    def accuracy(self, x, t):  #정확도
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y==t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):  #기울기
        loss_W = lambda W: self.loss(x,t)

        grads = {}  #기울기 보관하는 딕셔너리 변수
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads

#미니배치 학습 구현하기
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(X_train, t_train), (X_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []
train_acc_list = []
test_acc_list = []

#하이퍼파라미터
iters_num = 10000  #반복 횟수
train_size = X_train.shape[0]
batch_size = 100  #미니배치 크기
learning_rate = 0.1

network = TwoLayerNet(input_size= 784, hidden_size = 50, output_size= 10)

#1에폭당 반복 수
iter_per_epoch = max(train_size/batch_size, 1)

for i in range(iters_num):
    #미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)  #60000개의 훈련 데이터에서 임의로 100개의 데이터 추리기
    x_batch = X_train[batch_mask]
    t_batch = t_train[batch_mask]

    #기울기 계산
    grad = network.gradient(x_batch, t_batch)  #성능 개선판 : 오차역전파법
    #grad = network.numerical_gradient(x_batch, t_batch)

    #매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2' ):
        network.params[key] -= learning_rate *grad[key]

    #학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    #1에폭당 정확도 계산
    if i%iter_per_epoch ==0:
        train_acc = network.accuracy(X_train, t_train)
        test_acc = network.accuracy(X_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc : " + str(train_acc) +"," +str(test_acc))

# 그래프 그리기
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()