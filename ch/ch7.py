# -*- coding: utf-8 -*-
"""ch7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZxr2RfBRjRU3naaGPQkujnYlrL2f9aF
"""

import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import sys, os
# %cd /content/drive/MyDrive/Colab Notebooks/밑바닥딥러닝
from common.util import im2col

import pickle
import numpy as np
from collections import OrderedDict
from common.layers import *
from common.gradient import numerical_gradient

import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from simple_convnet import SimpleConvNet
from common.trainer import Trainer

"""합성곱/풀링 계층 구현하기"""

##4차원 배열
x = np.random.rand(10, 1, 28, 28)  #높이 28, 너비 28, 채널 1개인 데이터 10개 랜덤으로 생성
print(x.shape)
print(x)
#10개 중 첫번째 데이터에 접근
print(x[0].shape)  #(1, 28, 28)
print(x[1].shape)  #(1, 28, 28)

#첫번째 데이터의 첫 채널의 공간 데이터에 접근
x[0,0]

##합성곱 계층 구현하기
#im2col 사용
x1 = np.random.rand(1,3,7,7)  #(데이터수, 채널수, 높이, 너비)
col1 = im2col(x1, 5, 5, stride=1, pad=0)
print(col1.shape)  #(9, 75) -> 75: 필터의 원소 수 5(필터높이)*5(필터너비)*3(채널수)

x2 = np.random.rand(10,3,7,7)  #데이터 10개
col2 = im2col(x2, 5,5, stride=1, pad=0)
print(col2.shape)  #(90,75) -> 90: 배치 10이므로 배치 1일 때 9*10

class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad

    def forward(self,x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1+(H+2*self.pad -FH) / self.stride)
        out_W = int(1+(W+2*self.pad-FW)/ self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T  #필터 전개
        #(정수, -1) : 다차원 배열의 원소 수가 변환 후에도 유지되도록 적절히 묶어줌
        # -> (정수, 적절한 묶음 수) ex: 750개 원소 - reshape(10,-1) -> (10,75)
        #T 메소드 : 행과 열을 바꿔주는 메소드, 세로로 전개하기 위해 ex:(2,3) -> (3,2)
        out = np.dot(col, col_W) +self.b

        out = out.reshape(N, out_h, out_W, -1).transpose(0,3,1,2)
        #출력 데이터를 적절한 형상으로 바꿔주기
        #transpose메소드 : 인덱스를 지정하여 다차원 배열의 축 순서 바꿔주는 함수
        #-> (N, C(-1), out_h, out_W)

        return out

##풀링 계층 구현하기
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad

    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1+(H-self.pool_h)/self.stride)
        out_w = int(1+(W-self.pool_w)/self.stride)

        #전개
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
        #shape를 (적절한묶음, 풀영역원소개수)로 바꿔줌

        #최댓값
        out = np.max(col, axis = 1)  #행별 최댓값

        #성형
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        return out

"""CNN 구현하기"""

class SimpleConvNet:
    def __init__(self, input_dim=(1, 28,28),#채널수, 높이, 너비
                 conv_param={'filter_num':30, 'filter_size':5,'pad':0, 'stride':1},
                 hidden_size =100, output_size=10,weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]  #(높이, 너비)
        conv_output_size = (input_size - filter_size +2*filter_pad)/ filter_stride+1  #합성곱 계층의 출력 크기 계산
        pool_output_size = int(filter_num*(conv_output_size/2)*(conv_output_size/2))

        #가중치 매개변수 초기화
        self.params = {}
        self.params['W1']= weight_init_std*np.random.randn(filter_num, input_dim[0], filter_size, filter_size)  #합성곱계층 가중치
        self.params['b1'] = np.zeros(filter_num)
        self.params['W2'] = weight_init_std*np.random.randn(pool_output_size, hidden_size)  #완전연결계층 가중치
        self.params['b2'] = np.zeros(hidden_size)
        self.params['W3'] = weight_init_std*np.random.randn(hidden_size, output_size)  #2완전연결계층 가중치
        self.params['b3'] = np.zeros(output_size)

        #CNN 구성 계층
        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)

        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()

        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])
        self.last_layer = SoftmaxWithLoss()

    #추론 수행
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    #손실 함수
    def loss(self, x, t):
        y = self.predict(x)
        return self.last_layer.forward(y, t)

    #기울기 구하기
    def gradient(self, x, t):
        #순전파
        self.loss(x, t)

        #역전파
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        #결과 저장
        grads = {}
        grads['W1'] = self.layers['Conv1'].dW
        grads['b1'] = self.layers['Conv1'].db
        grads['W2'] = self.layers['Affine1'].dW
        grads['b2'] = self.layers['Affine1'].db
        grads['W3'] = self.layers['Affine2'].dW
        grads['b3'] = self.layers['Affine2'].db
        return grads

##MNIST 데이터셋으로 학습

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)

# 시간이 오래 걸릴 경우 데이터를 줄인다.
x_train, t_train = x_train[:5000], t_train[:5000]
x_test, t_test = x_test[:1000], t_test[:1000]

max_epochs = 20

network = SimpleConvNet(input_dim=(1,28,28),
                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},
                        hidden_size=100, output_size=10, weight_init_std=0.01)

trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=max_epochs, mini_batch_size=100,
                  optimizer='Adam', optimizer_param={'lr': 0.001},
                  evaluate_sample_num_per_epoch=1000)
trainer.train()

# 매개변수 보존
network.save_params("params.pkl")
print("Saved Network Parameters!")

# 그래프 그리기
markers = {'train': 'o', 'test': 's'}
x = np.arange(max_epochs)
plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)
plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

"""CNN시각화하기"""

##1번째 층의 가중치 시각화하기
def filter_show(filters, nx=8, margin=3, scale=10):
    """
    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py
    """
    FN, C, FH, FW = filters.shape
    ny = int(np.ceil(FN / nx))

    fig = plt.figure()
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

    for i in range(FN):
        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])
        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')
    plt.show()


network = SimpleConvNet()
# 무작위(랜덤) 초기화 후의 가중치
filter_show(network.params['W1'])

# 학습된 가중치
network.load_params("params.pkl")
filter_show(network.params['W1'])