# -*- coding: utf-8 -*-
"""ch2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17n_ngVWrxwtpxuSvLhYmsMcT2ocNEwfG
"""

import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import sys, os
# %cd /content/drive/MyDrive/Colab Notebooks/밑바닥딥러닝2

##코퍼스 전처리 함수
def preprocess(text):
    text = text.lower()
    text = text.replace('.', ' .')
    words = text.split(' ')  #공백기준으로 분할

    word_to_id = {}  #단어에서 단어ID로의 변환
    id_to_word = {}  #단어ID에서 단어로의 변환
    for word in words:
        if word not in word_to_id:  #새로운 ID와 단어 추가
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word

    corpus = np.array([word_to_id[w] for w in words])

    return corpus, word_to_id, id_to_word

text = 'You say goodbye and I say hello'

corpus, word_to_id, id_to_word = preprocess(text)
print(corpus)  #단어 ID 목록
print(word_to_id)
print(id_to_word)

##말뭉치로부터 동시발생 행렬 만들어주는 함수
def create_co_matrix(corpus, vocab_size, window_size=1):
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype = np.int32)

    for idx, word_id in enumerate(corpus):  #인덱스와 word_id(corpus)로 이루어진 튜플 생성
        for i in range(1, window_size+1):
            left_idx = idx-i
            right_idx = idx +i

            if left_idx >=0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] +=1

            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id]+=1

    return co_matrix

##코사인 유사도 함수
def cos_similarity(x,y, eps=1e-8):  #0으로 나누는 오류 막기위해
    nx = x / np.sqrt(np.sum(x**2)+eps)  #x의 정규화
    ny = y/ np.sqrt(np.sum(y**2)+eps)  #y의 정규화
    return np.dot(nx, ny)

##you와 I의 유사도 구하는 코드
from common.util import preprocess, create_co_matrix, cos_similarity
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

c0 = C[word_to_id['you']]  #you의 단어 벡터
c1 = C[word_to_id['i']]  #i의 단어 벡터
print(cos_similarity(c0, c1))

##유사 단어의 랭킹 표시
def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):  #query :검색어(단어)
    #1. 검색어를 꺼낸다
    if query not in word_to_id:
        print('%s(을)를 찾을 수 없습니다.' %query)
        return

    print('\n[query] '+query)
    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]

    #2. 코사인 유사도 계산
    vocab_size = len(id_to_word)
    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)

    #3. 코사인 유사도를 기준으로 내림차순으로 출력
    count = 0
    for i in (-1*similarity).argsort():
        #argsort() : 넘파이 배열의 원소를 오름차순으로 정렬 후 배열의 인덱스 반환
        if id_to_word[i] == query:
            continue
        print(' %s: %s' % (id_to_word[i], similarity[i]))

        count +=1
        if count >= top:
            return

##you를 검색어로 유사 단어 출력
from common.util import preprocess, create_co_matrix, most_similar

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

most_similar('you', word_to_id, id_to_word, C, top=5)

"""통계 기반 기법 개선하기"""

##양의 상호정보량 PPMI
#동시발생 행렬을 PPMI 행렬로 변환하는 함수
def ppmi(C, verbose=False, eps = 1e-8):
    M = np.zeros_like(C, dtypes=np.float32)
    N = np.sum(C)  #근삿값
    S = np.sum(C, axis=0)
    total = C.shape[0]*C.shape[1]
    cnt = 0

    for i in range(C.shape[0]):
        for j in range(C.shape[1]):
            pmi = np.log2(C[i,j]*N/(S[j]*S[i])+eps)
            #음의 무한대로 빠지지 않기 위해 eps 더해줌
            M[i,j] = max(0, pmi)  #음수일 때는 0으로 취급

            if verbose:  #진행상황 출력
                cnt +=1
                if cnt % (total//100+1) ==0:
                    print('%.1f%% 완료' % (100*cnt/total))

    return M

##변환해보기
from common.util import preprocess, create_co_matrix, cos_similarity, ppmi

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)
W = ppmi(C)  #각 원소는 0이상의 실수

np.set_printoptions(precision=3)  #유효 자릿수를 세 자리로 표시
print('동시발생 행렬')
print(C)
print('-'*50)
print('PPMI')
print(W)

##SVD에 의한 차원 감소
import matplotlib.pyplot as plt

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)
W = ppmi(C)

#SVD - 넘파이의 linalg모듈이 제공하는 svd메소드 사용
U, S, V = np.linalg.svd(W)

print(C[0])  #동시발생 행렬
print(W[0])  #PPMI 행렬
print(U[0])  #SVD
print(U[0, :2])  #각 단어를 2차원 벡터로 줄임

##PTB데이터셋
import sys
sys.path.append('..')
from dataset import ptb

corpus, word_to_id, id_to_word = ptb.load_data('train')
#데이터 읽기, train/test/valid 중 하나 지정 가능

print('말뭉치 크기 :', len(corpus))
print('corpus[:30]', corpus[:30])
print()
print('id_to_word[0]:', id_to_word[0])
print('id_to_word[1]:', id_to_word[1])
print('id_to_word[2]:', id_to_word[2])
print()
print("word_to_id['car']:", word_to_id['car'])
print("word_to_id['happy']:", word_to_id['happy'])
print("word_to_id['lexus']:", word_to_id['lexus'])

##PTB 데이터셋 평가

window_size = 2
wordvec_size = 100

corpus, word_to_id, id_to_word = ptb.load_data('train')
#데이터 읽기, train/test/valid 중 하나 지정 가능
vocab_size = len(word_to_id)
print('동시발생 수 계산...')
C = create_co_matrix(corpus, vocab_size, window_size)
print('PPMI 계산...')
W = ppmi(C, verbose = True)

print('SVD 계산...')
try:
    # truncated SVD
    from sklearn.utils.extmath import randomized_svd
    #sklearn의 고속SVD, 무작위 수를 사용, 특잇값이 큰 것들만 계산하여 기본적인 SVD보다 훨씬 빠름
    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,
                             random_state=None)
except:
    # SVD
    U, S, V = np.linalg.svd(W)


word_vecs = U[:, :wordvec_size]
querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)